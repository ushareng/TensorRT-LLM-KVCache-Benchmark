# tensorrt-llm-kv-cache-benchmark
Benchmarking and comparing TensorRT-LLM inference performance with and without paged KV cache, using synthetic fixed-length token inputs to evaluate latency and throughput across different sequence lengths and batch sizes.
